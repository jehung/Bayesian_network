For the Gibbs (and Metropolis-Hastings) sampling, the acceptance criterion seems way too loose. It is easy to get stuck in a sampling "hole" for 10 iterations and end up with a really crummy estimation for 10 iterations in a row. A comparison threshold a few orders of magnitude smaller (say, 0.0001) seems more interesting, especially to compare the two.

- Yes, the convergence criteria has been kept loose. You can change the threshold (delta) to anything <= 0.1, if you want. Doing so might also improve the converged posterior.

What about "burn in" time. That is, early iterations of the sampling algorithms may not be capturing realistic states at a proportional level. Are we allowed to run the initial state through the algorithm for a few hundred iterations to give it a better starting point?
- Yes, you can and should run the initial state a few hundred/thousand times, but do include it in your final count of iterations returned.

Going back to the acceptance conditions - most writeups use a minimum (or fixed) number of iterations to run the sampling algorithms. Should we be setting a minimum? Technically, we can start testing after the first iteration, but there is a good possibility that after 11 iterations the state space has not been adequately explored and the acceptance criteria are met (too early).
- Yes, you can set a minimum.. go ahead and experiment with different values.

One thing that isn't present in the signatures of the single iteration sampling functions (Gibbs_sampling and MH_sampling) is something to indicate what elements are fixed (known). I added a keyword argument with a default, but I wanted to make sure this is ok. Is there some other way that I'm not seeing to communicate which parts of the state should not be modified?
- You don't know which elements are fixed (known) at this point.. So, given a state and the underlying probability distributions, you have to come up with a new state, by randomly sampling any of the given values.

I have a question about the inference engine, which might be just me not understanding the probability theory correctly. Taking the game example used in the assignment, If A and B are known, then the game AvB's distribution is fixed as that given by the provided probability table, right? Because when I set up the Bayes Net (with 3 participants), the probabilities reported by the inference engine for AvB with fixed A and B are not matching the input table. Moreover, AvB seems independent of things that it should be dependent on, and independent of others that shouldn't matter. I've checked my network setup many times, but it is possible I'm getting something wrong here. Is anyone else seeing something similar?
- That shouldn't happen.

For MH sampling, one iteration means one sample that may or may not be rejected, right? I'm checking for rejection in my MH_sampling function, and return None if it is rejected. Is this ok? You can't reject and return the previous sample, because that will misrepresent the previous sample as getting over-represented.
- You mustn't return None.

Is an iteration of Gibbs ?
(a) - one sample of one variable resampled at random
(b) - a complete iteration over all variables not held constant.

- The correct answer is (a)
 Example: Given the [0..0] vector in the starter code a single call to gibb_sampling should pick one of the nodes at random and update its results. As a result on the 10-vector of zeros, the returned results would be nine zeros and one value that is potentially changed. If the index for T3 was chosen, only that variable would be updated (lets say to '2' and thus the returned value would be (0,0,2,0...0)).

How will 2c and 2d be graded?
- 2c: Correctness of a new sample based on previous sample (differing in atmost 1 position)
2d: Completeness of the method to be able to converge to any posterior, preferably close to correct (but not necessary!)